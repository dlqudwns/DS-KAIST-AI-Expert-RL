{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_exercise_2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGwJw8UlVp7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2\n",
        "\n",
        "### Animation 관련 추가 패키지 ###\n",
        "!pip install box2d-py mako==1.0.7 Pygame JSAnimation imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5bsLvVLBiW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/secury/DS-KAIST-AI-Expert-RL.git\n",
        "!pip install pygame\n",
        "%cd DS-KAIST-AI-Expert-RL/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOug-AE7Cpci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import gym\n",
        "import envs\n",
        "import numpy as np\n",
        "\n",
        "# Library related to Java Script Animation\n",
        "from matplotlib import animation\n",
        "from JSAnimation import IPython_display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsmFzVYzCpRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LeKTl12BpQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=True, threshold=10000, linewidth=250)\n",
        "\n",
        "\"\"\" Load environment \"\"\"\n",
        "env_name = 'MazeSample5x5-v0'\n",
        "# env_name = 'MazeSample10x10-v0'\n",
        "# env_name = 'MazeRandom10x10-v0'\n",
        "# env_name = 'MazeRandom10x10-plus-v0'\n",
        "# env_name = 'MazeRandom20x20-v0'\n",
        "# env_name = 'MazeRandom20x20-plus-v0'\n",
        "# env_name = 'MyCartPole-v0'\n",
        "# env_name = 'MyMountainCar-v0'\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = env.unwrapped\n",
        "\"\"\"\n",
        "env.S: the number of states (integer)\n",
        "env.A: the number of actions (integer)\n",
        "env.T: transition matrix (S x A x S)-sized array\n",
        "env.R: reward matrix (S x A)-sized array\n",
        "env.gamma: discount factor (0 ~ 1)\n",
        "\"\"\"\n",
        "        \n",
        "\n",
        "def plot_movie_js(image_array):\n",
        "    dpi = 10.0\n",
        "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
        "    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n",
        "    # fig.suptitle(filename, fontsize=160)\n",
        "    # fig.set_xlabel(filename, fontsize=160)\n",
        "    # fig.xlabel(filename, fontsize=160)\n",
        "    im = plt.figimage(image_array[0])\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_array(image_array[i])\n",
        "        return (im,)\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
        "    ipythondisplay.display(IPython_display.display_animation(anim))\n",
        "\n",
        "\n",
        "def policy_evaluation(env, pi):\n",
        "    \"\"\"\n",
        "    :param env: MDP(S, A, T, R, gamma)\n",
        "    :param pi: behavior policy (S x A)-sized array\n",
        "    :return: V, Q where V is (S)-sized array and Q is (S x A)-sized array\n",
        "    \"\"\"\n",
        "    r = np.sum(env.R * pi, axis=1)\n",
        "    P = np.tensordot(pi, env.T, axes=([1], [1]))[np.arange(env.S), np.arange(env.S), :]\n",
        "    V = np.linalg.inv(np.eye(env.S) - env.gamma * P).dot(r)\n",
        "    Q = env.R + env.gamma * env.T.dot(V)\n",
        "\n",
        "    return V, Q\n",
        "\n",
        "\n",
        "def policy_improvement(env, Q):\n",
        "    pi = np.zeros((env.S, env.A))\n",
        "    pi[np.arange(env.S), np.argmax(Q, axis=1)] = 1.\n",
        "    return pi\n",
        "\n",
        "\n",
        "def policy_iteration(env):\n",
        "    pi = np.ones((env.S, env.A)) / env.A\n",
        "    for i in range(1000):\n",
        "        V, Q = policy_evaluation(env, pi)\n",
        "        new_pi = policy_improvement(env, Q)\n",
        "        if np.all(pi == new_pi):\n",
        "            break\n",
        "        pi = new_pi\n",
        "    return pi, Q\n",
        "\n",
        "\n",
        "def value_iteration(env):\n",
        "    V, Q = np.zeros(env.S), np.zeros((env.S, env.A))\n",
        "    for i in range(1000):\n",
        "        Q_new = env.R + env.gamma * env.T.dot(V)\n",
        "        V_new = np.max(Q_new, axis=1)\n",
        "        if np.max(np.abs(V - V_new)) < 1e-6:\n",
        "            break\n",
        "        V, Q = V_new, Q_new\n",
        "\n",
        "    pi = np.zeros((env.S, env.A))\n",
        "    pi[np.arange(env.S), np.argmax(Q, axis=1)] = 1.\n",
        "\n",
        "    return pi, Q\n",
        "\n",
        "\n",
        "pi, Q = policy_iteration(env)\n",
        "\n",
        "for episode in range(1):\n",
        "    state = env.reset()\n",
        "    render_list = []\n",
        "    episode_reward = 0.\n",
        "\n",
        "    for t in range(10000):\n",
        "        action = int(np.random.choice(np.arange(env.A), p=pi[state, :]))\n",
        "        state1, reward, done, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        print(\"[%4d] state=%4s / action=%d / reward=%7.4f / state1=%4s / info=%s\" % (t, state, action, reward, state1, info))\n",
        "\n",
        "        env.draw_policy_evaluation(Q, pi)  # 필요시 주석 처리\n",
        "        screen = env.render(mode='rgb_array')\n",
        "        render_list.append(screen)\n",
        "        time.sleep(0.3 if 'Maze' in env_name else 0.01)\n",
        "\n",
        "        if done:\n",
        "            plot_movie_js(render_list)\n",
        "            break\n",
        "        state = state1\n",
        "    print('Episode reward: %.4f' % episode_reward)\n",
        "\n",
        "    time.sleep(1)\n",
        "time.sleep(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9EiLpPIBzOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}