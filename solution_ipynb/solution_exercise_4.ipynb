{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "solution_exercise_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLtj2lpHgTG1",
        "colab_type": "text"
      },
      "source": [
        "## 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc15JXK1QKhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### 환경설정 ###\n",
        "!rm -rf sample_data\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2\n",
        "!pip install pygame\n",
        "\n",
        "### Animation 관련 추가 패키지 ###\n",
        "!pip install box2d-py mako==1.0.7 JSAnimation imageio\n",
        "\n",
        "### Code 받아오기 ###\n",
        "!git clone https://github.com/secury/DS-KAIST-AI-Expert-RL.git\n",
        "\n",
        "%cd DS-KAIST-AI-Expert-RL/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-xxQXFAgVhe",
        "colab_type": "text"
      },
      "source": [
        "## Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVyE5gCIQPRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "import gym\n",
        "import envs\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Library related to Java Script Animation\n",
        "from matplotlib import animation\n",
        "from JSAnimation import IPython_display\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True, threshold=10000, linewidth=250)\n",
        "\n",
        "        \n",
        "def plot_movie_js(image_array):\n",
        "    dpi = 10.0\n",
        "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
        "    fig = plt.figure(figsize=(ypixels/(dpi), xpixels/(dpi)), dpi=dpi)\n",
        "    # fig.suptitle(filename, fontsize=160)\n",
        "    # fig.set_xlabel(filename, fontsize=160)\n",
        "    # fig.xlabel(filename, fontsize=160)\n",
        "    im = plt.figimage(image_array[0])\n",
        "\n",
        "    def animate(i):\n",
        "        im.set_array(image_array[i])\n",
        "        return (im,)\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
        "    ipythondisplay.display(IPython_display.display_animation(anim))\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_dim, action_size, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        self.q_model = self._build_model()\n",
        "        self.q_model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
        "        self.q_model.predict_one = lambda x: self.q_model.predict(np.array([x]))[0]\n",
        "        self.target_q_model = self._build_model()\n",
        "        self.target_q_model.predict_one = lambda x: self.target_q_model.predict(np.array([x]))[0]\n",
        "\n",
        "        self.update_target_q_weights()  # target Q network 의 파라미터를 Q-newtork 에서 복사\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = Sequential([\n",
        "            Dense(64, input_dim=self.state_dim, activation='relu'),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def update_target_q_weights(self):\n",
        "        #################################\n",
        "        # TODO:\n",
        "        self.target_q_model.set_weights(self.q_model.get_weights())\n",
        "        #################################\n",
        "\n",
        "    def act(self, state):\n",
        "        # epsilon-greedy policy\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.randint(0, self.action_size)\n",
        "        else:\n",
        "            q_values = self.q_model.predict_one(state)\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self):\n",
        "        if len(agent.memory) < self.batch_size:\n",
        "            return\n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "        input_state_batch, target_q_values_batch = [], []\n",
        "\n",
        "        for state, action, reward, next_state, done in mini_batch:\n",
        "            q_values = self.q_model.predict_one(state)\n",
        "            \n",
        "            #################################\n",
        "            # TODO:\n",
        "            if done:\n",
        "                q_values[action] = reward\n",
        "            else:\n",
        "                q_values[action] = reward + self.gamma * np.max(self.target_q_model.predict_one(next_state))\n",
        "            ##################################\n",
        "            \n",
        "            \n",
        "            input_state_batch.append(state)\n",
        "            target_q_values_batch.append(q_values)\n",
        "\n",
        "        # Q-network 학습\n",
        "        self.q_model.fit(np.array(input_state_batch), np.array(target_q_values_batch), batch_size=self.batch_size, epochs=1)\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = np.max([self.epsilon * self.epsilon_decay, self.epsilon_min])\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Load environment \"\"\"\n",
        "env_name = 'CartPole-v0'\n",
        "# env_name = 'MyPendulum-v0'\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = env.unwrapped\n",
        "env.T = env.R = None\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(state_dim, action_size, gamma=0.99)\n",
        "num_episodes = 5000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0.\n",
        "    render_list = []\n",
        "\n",
        "    if episode % 100 == 0 or episode == num_episodes-1:\n",
        "        print('')\n",
        "        print('Episode ' + str(episode) + ':')\n",
        "\n",
        "    for t in range(10000):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Replay buffer 에 (s,a,r,s') 저장\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if episode % 100 == 0 or episode == num_episodes-1:\n",
        "            # print(\"[epi=%4d,t=%4d] state=%4s / action=%s / reward=%7.4f / next_state=%4s / Q[s]=%s\" % (episode, t, state, action, reward, next_state, agent.q_model.predict_one(state)))\n",
        "            screen = env.render(mode='rgb_array')\n",
        "            render_list.append(screen)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    # 100 에피소드 마다 지금까지 학습된 Q-network에 따른 trajectory를 무비클립으로 확인\n",
        "    if episode % 100 == 0 or episode == num_episodes-1:\n",
        "        # 펜듈럼 실행시 아래 두 라인을 주석 해제 시켜주시고 세번째 라인을 주석처리 시켜주세요.\n",
        "        # if episode != 0:\n",
        "        #     plot_movie_js(render_list)\n",
        "        plot_movie_js(render_list)\n",
        "        print('[%4d] Episode reward=%.4f / epsilon=%f' % (episode, episode_reward, agent.epsilon))\n",
        "        print()\n",
        "\n",
        "    # 에피소드가 끝날 때마다 Q-network 을 학습하고, epsilon 을 점차 낮춘다.\n",
        "    agent.train()\n",
        "    agent.update_epsilon()\n",
        "\n",
        "    # 에피소드마다 10번마다 target network 의 파라미터를 현재 Q-network 파라미터로 갱신해준다.\n",
        "    if episode % 10 == 0:\n",
        "        agent.update_target_q_weights()\n",
        "\n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}